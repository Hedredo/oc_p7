{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook est destin√© √† calculer les performances d'un mod√®le √† base de r√©seaux de neurones bas√© sur diff√©rentes architectures avec des poids pr√©-entrain√©s ou pas.<br>\n",
    "Pour des raisons li√©es √† une instabilit√© m√©moire du GPU sur cet environnement de test, chaque exp√©rience rechargera un environnement apr√®s red√©marrage du kernel pour √©viter tout crash.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EXPERIMENT SCRATCH**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Global Average Pooling Run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 19:12:23.203522: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-05 19:12:24.705507: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdirectml.d6f03b303ac3c4f2eeb8ca631688c9757b361310.so\n",
      "2025-01-05 19:12:24.705573: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdxcore.so\n",
      "2025-01-05 19:12:24.710590: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libd3d12.so\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "2025-01-05 19:12:25.288729: I tensorflow/c/logging.cc:34] DirectML device enumeration: found 1 compatible adapters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 314\n",
      "Tensorflow framework: GPU is available\n"
     ]
    }
   ],
   "source": [
    "# Please restart the kernel before running this cell to free up GPU memory\n",
    "%run nn_env_a.ipynb\n",
    "\n",
    "# Define the URI of the MLflow server and the name of the experiment\n",
    "run = \"GlobalAveragePooling1D\"\n",
    "\n",
    "# Args for dataset preparation\n",
    "col_name = \"text\"\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "# Args for the model\n",
    "max_tokens = 10000\n",
    "seq_length = 100\n",
    "embedding_dim = 16\n",
    "embedding_trainable = True\n",
    "epochs = 20\n",
    "layers = (tf.keras.layers.GlobalAveragePooling1D(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 19:12:33.003355: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-05 19:12:33.004801: I tensorflow/c/logging.cc:34] DirectML: creating device on adapter 0 (AMD Radeon RX 6700 XT)\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "2025-01-05 19:12:33.091568: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-01-05 19:12:33.091604: W tensorflow/core/common_runtime/pluggable_device/pluggable_device_bfc_allocator.cc:37] Ignoring the value of TF_FORCE_GPU_ALLOW_GROWTH because force_memory_growth was requested by the device.\n",
      "2025-01-05 19:12:33.091625: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2025-01-05 19:12:33.309536: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025/01/05 19:12:34 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'ascii' codec can't decode byte 0xc2 in position 26: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  10000\n",
      "Epoch 1/20\n",
      "  1/479 [..............................] - ETA: 3:29 - loss: 0.6968 - binary_accuracy: 0.3125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 19:12:34.815253: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-01-05 19:12:34.882288: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-01-05 19:12:34.882342: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2025-01-05 19:12:34.883984: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-01-05 19:12:34.884027: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2025-01-05 19:12:34.886295: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-01-05 19:12:34.886338: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/479 [..............................] - ETA: 10s - loss: 0.6943 - binary_accuracy: 0.4141 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0056s vs `on_train_batch_end` time: 0.0185s). Check your callbacks.\n",
      "477/479 [============================>.] - ETA: 0s - loss: 0.6902 - binary_accuracy: 0.5448"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 19:12:45.006526: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-01-05 19:12:45.027028: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-01-05 19:12:45.027069: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479/479 [==============================] - 11s 22ms/step - loss: 0.6902 - binary_accuracy: 0.5449 - val_loss: 0.6857 - val_binary_accuracy: 0.6730\n",
      "Epoch 2/20\n",
      "479/479 [==============================] - 8s 16ms/step - loss: 0.6767 - binary_accuracy: 0.6440 - val_loss: 0.6684 - val_binary_accuracy: 0.6641\n",
      "Epoch 3/20\n",
      "479/479 [==============================] - 10s 22ms/step - loss: 0.6505 - binary_accuracy: 0.6885 - val_loss: 0.6437 - val_binary_accuracy: 0.6780\n",
      "Epoch 4/20\n",
      "479/479 [==============================] - 11s 22ms/step - loss: 0.6189 - binary_accuracy: 0.7109 - val_loss: 0.6211 - val_binary_accuracy: 0.6931\n",
      "Epoch 5/20\n",
      "479/479 [==============================] - 8s 16ms/step - loss: 0.5890 - binary_accuracy: 0.7294 - val_loss: 0.6030 - val_binary_accuracy: 0.6965\n",
      "Epoch 6/20\n",
      "479/479 [==============================] - 10s 21ms/step - loss: 0.5624 - binary_accuracy: 0.7440 - val_loss: 0.5883 - val_binary_accuracy: 0.7017\n",
      "Epoch 7/20\n",
      "479/479 [==============================] - 11s 23ms/step - loss: 0.5382 - binary_accuracy: 0.7598 - val_loss: 0.5762 - val_binary_accuracy: 0.7111\n",
      "Epoch 8/20\n",
      "479/479 [==============================] - 8s 16ms/step - loss: 0.5161 - binary_accuracy: 0.7746 - val_loss: 0.5660 - val_binary_accuracy: 0.7176\n",
      "Epoch 9/20\n",
      "479/479 [==============================] - 10s 21ms/step - loss: 0.4956 - binary_accuracy: 0.7896 - val_loss: 0.5577 - val_binary_accuracy: 0.7289\n",
      "Epoch 10/20\n",
      "479/479 [==============================] - 10s 22ms/step - loss: 0.4767 - binary_accuracy: 0.8016 - val_loss: 0.5511 - val_binary_accuracy: 0.7312\n",
      "Epoch 11/20\n",
      "479/479 [==============================] - 8s 16ms/step - loss: 0.4592 - binary_accuracy: 0.8114 - val_loss: 0.5461 - val_binary_accuracy: 0.7307\n",
      "Epoch 12/20\n",
      "479/479 [==============================] - 10s 21ms/step - loss: 0.4431 - binary_accuracy: 0.8196 - val_loss: 0.5424 - val_binary_accuracy: 0.7328\n",
      "Epoch 13/20\n",
      "479/479 [==============================] - 10s 22ms/step - loss: 0.4282 - binary_accuracy: 0.8284 - val_loss: 0.5399 - val_binary_accuracy: 0.7328\n",
      "Epoch 14/20\n",
      "479/479 [==============================] - 9s 19ms/step - loss: 0.4143 - binary_accuracy: 0.8359 - val_loss: 0.5386 - val_binary_accuracy: 0.7359\n",
      "Epoch 15/20\n",
      "479/479 [==============================] - 10s 21ms/step - loss: 0.4014 - binary_accuracy: 0.8416 - val_loss: 0.5382 - val_binary_accuracy: 0.7351\n",
      "Epoch 16/20\n",
      "479/479 [==============================] - 10s 22ms/step - loss: 0.3893 - binary_accuracy: 0.8469 - val_loss: 0.5386 - val_binary_accuracy: 0.7370\n",
      "Epoch 17/20\n",
      "479/479 [==============================] - 8s 16ms/step - loss: 0.3780 - binary_accuracy: 0.8528 - val_loss: 0.5397 - val_binary_accuracy: 0.7380\n",
      "Epoch 18/20\n",
      "479/479 [==============================] - 11s 22ms/step - loss: 0.3672 - binary_accuracy: 0.8584 - val_loss: 0.5415 - val_binary_accuracy: 0.7380\n",
      "1/1 [==============================] - 0s 65ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 19:15:27.285910: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6z_iq5bt/model/data/model/assets\n",
      "Evaluate on test data\n",
      "==============\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.5383 - binary_accuracy: 0.7480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/05 19:15:32 INFO mlflow.tracking._tracking_service.client: üèÉ View run GlobalAveragePooling1D at: http://localhost:5000/#/experiments/4/runs/486c6beca0a441f982db1ce1b3aa895e.\n",
      "2025/01/05 19:15:32 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://localhost:5000/#/experiments/4.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextVectorizer.__init__() got an unexpected keyword argument 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 51\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Log the additionnal metrics & parameters\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# mlflow.log_metrics({\"val_loss\": val_loss, \"val_accuracy\": val_accuracy, \"inference_time\": inference_time})\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# mlflow.log_params({\"data_preparation\": col_name, \"test_size_ratio\": test_split, \"val_splits\": len(val_scores[\"test_accuracy\"])})\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Evaluate the data on the test set with th model logged in MLflow\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# evaluation_data = pd.concat([X_test, y_test], axis=1).assign(predictions=y_pred)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m model_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns:/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactive_run\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclassifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluator_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_model_explainability\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Disable SHAP explanations\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/mlflow/models/evaluation/base.py:1741\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data, model_type, targets, predictions, dataset_path, feature_names, evaluators, evaluator_config, custom_metrics, extra_metrics, custom_artifacts, validation_thresholds, baseline_model, env_manager, model_config, baseline_config, inference_params)\u001b[0m\n\u001b[1;32m   1739\u001b[0m         model \u001b[38;5;241m=\u001b[39m _get_model_from_deployment_endpoint_uri(model, inference_params)\n\u001b[1;32m   1740\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1741\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_model_or_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m env_manager \u001b[38;5;241m!=\u001b[39m _EnvManager\u001b[38;5;241m.\u001b[39mLOCAL:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m   1744\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model argument must be a string URI referring to an MLflow model when a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-local env_manager is specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1746\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mINVALID_PARAMETER_VALUE,\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py:1151\u001b[0m, in \u001b[0;36m_load_model_or_server\u001b[0;34m(model_uri, env_manager, model_config)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyfunc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscoring_server\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m   1146\u001b[0m     ScoringServerClient,\n\u001b[1;32m   1147\u001b[0m     StdinScoringServerClient,\n\u001b[1;32m   1148\u001b[0m )\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env_manager \u001b[38;5;241m==\u001b[39m _EnvManager\u001b[38;5;241m.\u001b[39mLOCAL:\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting model server for model environment restoration.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1155\u001b[0m local_path \u001b[38;5;241m=\u001b[39m _download_artifact_from_uri(artifact_uri\u001b[38;5;241m=\u001b[39mmodel_uri)\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/mlflow/tracing/provider.py:268\u001b[0m, in \u001b[0;36mtrace_disabled.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m disable()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     is_func_called, result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     enable()\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py:1052\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_uri, suppress_warnings, dst_path, model_config)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         model_impl \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(conf[MAIN])\u001b[38;5;241m.\u001b[39m_load_pyfunc(data_path, model_config)\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1052\u001b[0m         model_impl \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mMAIN\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pyfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;66;03m# This error message is particularly for the case when the error is caused by module\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;66;03m# \"databricks.feature_store.mlflow_model\". But depending on the environment, the offending\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;66;03m# module might be \"databricks\", \"databricks.feature_store\" or full package. So we will\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;66;03m# raise the error with the following note if \"databricks\" presents in the error. All non-\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;66;03m# databricks moduel errors will just be re-raised.\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m conf[MAIN] \u001b[38;5;241m==\u001b[39m _DATABRICKS_FS_LOADER_MODULE \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatabricks\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/mlflow/tensorflow/__init__.py:726\u001b[0m, in \u001b[0;36m_load_pyfunc\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;66;03m# In SavedModel format, loaded model should be compiled.\u001b[39;00m\n\u001b[1;32m    725\u001b[0m     should_compile \u001b[38;5;241m=\u001b[39m save_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 726\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[43m_load_keras_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeras_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeras_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshould_compile\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _KerasModelWrapper(m, model_meta\u001b[38;5;241m.\u001b[39msignature)\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m _MODEL_TYPE_TF1_ESTIMATOR:\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/mlflow/tensorflow/__init__.py:564\u001b[0m, in \u001b[0;36m_load_keras_model\u001b[0;34m(model_path, keras_module, save_format, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m keras_models\u001b[38;5;241m.\u001b[39mload_model(model_path, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;66;03m# NOTE: Older versions of Keras only handle filepath.\u001b[39;00m\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkeras_models\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/keras/engine/base_layer.py:828\u001b[0m, in \u001b[0;36mLayer.from_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_config\u001b[39m(\u001b[38;5;28mcls\u001b[39m, config):\n\u001b[1;32m    814\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a layer from its config.\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \n\u001b[1;32m    816\u001b[0m \u001b[38;5;124;03m    This method is the reverse of `get_config`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;124;03m        A layer instance.\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: TextVectorizer.__init__() got an unexpected keyword argument 'name'"
     ]
    }
   ],
   "source": [
    "# Create the tensorflow dataset\n",
    "train_ds, val_ds, test_ds = dl.to_tensorflow_dataset(\n",
    "    col_name,\n",
    "    PATH_PARQUET,\n",
    "    X_train_full,\n",
    "    X_test_full,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    val_split,\n",
    "    batch_size,\n",
    ")\n",
    "# Start the MLflow run & autolog\n",
    "mlflow.tensorflow.autolog(checkpoint=False, log_models=True)\n",
    "with mlflow.start_run(run_name=run) as active_run:\n",
    "    # Create & build the model with defined parameters\n",
    "    model = dl.create_tf_model(\n",
    "        max_tokens, seq_length, None, embedding_dim, layers, train_ds\n",
    "    )\n",
    "    model.get_layer(\"embedding\").trainable = embedding_trainable\n",
    "    model.build(input_shape=(None, 1))\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluate on test data\")\n",
    "    print(\"==============\")\n",
    "    start_time = time.time()\n",
    "    loss, accuracy = model.evaluate(test_ds)\n",
    "    inference_time = time.time() - start_time\n",
    "    mlflow.log_metric(\"test_loss\", loss)\n",
    "    mlflow.log_metric(\"test_accuracy\", accuracy)\n",
    "\n",
    "    mlflow.log_param(\"batch_size_\", batch_size)\n",
    "    mlflow.log_param(\"validation_split_\", val_split)\n",
    "    # Log the additionnal metrics & parameters\n",
    "    # mlflow.log_metrics({\"val_loss\": val_loss, \"val_accuracy\": val_accuracy, \"inference_time\": inference_time})\n",
    "    # mlflow.log_params({\"data_preparation\": col_name, \"test_size_ratio\": test_split, \"val_splits\": len(val_scores[\"test_accuracy\"])})\n",
    "\n",
    "    # Evaluate the data on the test set with th model logged in MLflow\n",
    "    # evaluation_data = pd.concat([X_test, y_test], axis=1).assign(predictions=y_pred)\n",
    "    model_uri = f\"runs:/{active_run.info.run_id}/model\"\n",
    "    mlflow.evaluate(\n",
    "        model=model_uri,\n",
    "        model_type=\"classifier\",\n",
    "        data=test_ds,\n",
    "        targets=y_test,\n",
    "        evaluators=None,\n",
    "        evaluator_config={\n",
    "            \"log_model_explainability\": False\n",
    "        },  # Disable SHAP explanations\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 18:53:01.357576: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-05 18:53:01.359070: I tensorflow/c/logging.cc:34] DirectML: creating device on adapter 0 (AMD Radeon RX 6700 XT)\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "2025-01-05 18:53:01.438897: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-01-05 18:53:01.438929: W tensorflow/core/common_runtime/pluggable_device/pluggable_device_bfc_allocator.cc:37] Ignoring the value of TF_FORCE_GPU_ALLOW_GROWTH because force_memory_growth was requested by the device.\n",
      "2025-01-05 18:53:01.438949: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2025-01-05 18:53:01.659535: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025/01/05 18:53:02 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'ascii' codec can't decode byte 0xc2 in position 26: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  10000\n",
      "Epoch 1/20\n",
      "  1/479 [..............................] - ETA: 3:20 - loss: 0.6917 - binary_accuracy: 0.6875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 18:53:03.206742: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-01-05 18:53:03.258409: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-01-05 18:53:03.258456: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2025-01-05 18:53:03.260134: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-01-05 18:53:03.260174: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2025-01-05 18:53:03.262509: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-01-05 18:53:03.262547: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/479 [..............................] - ETA: 9s - loss: 0.6922 - binary_accuracy: 0.5859  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0055s vs `on_train_batch_end` time: 0.0178s). Check your callbacks.\n",
      "477/479 [============================>.] - ETA: 0s - loss: 0.6900 - binary_accuracy: 0.5481"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 18:53:13.254556: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-01-05 18:53:13.274288: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-01-05 18:53:13.274329: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479/479 [==============================] - 11s 22ms/step - loss: 0.6900 - binary_accuracy: 0.5481 - val_loss: 0.6852 - val_binary_accuracy: 0.6673\n",
      "Epoch 2/20\n",
      "479/479 [==============================] - 10s 22ms/step - loss: 0.6756 - binary_accuracy: 0.6453 - val_loss: 0.6669 - val_binary_accuracy: 0.6631\n",
      "Epoch 3/20\n",
      "479/479 [==============================] - 8s 16ms/step - loss: 0.6487 - binary_accuracy: 0.6902 - val_loss: 0.6422 - val_binary_accuracy: 0.6780\n",
      "Epoch 4/20\n",
      "479/479 [==============================] - 10s 22ms/step - loss: 0.6172 - binary_accuracy: 0.7112 - val_loss: 0.6200 - val_binary_accuracy: 0.6921\n",
      "Epoch 5/20\n",
      "479/479 [==============================] - 10s 21ms/step - loss: 0.5877 - binary_accuracy: 0.7300 - val_loss: 0.6023 - val_binary_accuracy: 0.6960\n",
      "Epoch 6/20\n",
      "479/479 [==============================] - 8s 16ms/step - loss: 0.5614 - binary_accuracy: 0.7450 - val_loss: 0.5878 - val_binary_accuracy: 0.7020\n",
      "Epoch 7/20\n",
      "479/479 [==============================] - 11s 23ms/step - loss: 0.5375 - binary_accuracy: 0.7603 - val_loss: 0.5758 - val_binary_accuracy: 0.7122\n",
      "Epoch 8/20\n",
      "479/479 [==============================] - 10s 21ms/step - loss: 0.5154 - binary_accuracy: 0.7754 - val_loss: 0.5658 - val_binary_accuracy: 0.7187\n",
      "Epoch 9/20\n",
      "479/479 [==============================] - 8s 16ms/step - loss: 0.4951 - binary_accuracy: 0.7906 - val_loss: 0.5575 - val_binary_accuracy: 0.7291\n",
      "Epoch 10/20\n",
      "479/479 [==============================] - 10s 21ms/step - loss: 0.4762 - binary_accuracy: 0.8020 - val_loss: 0.5510 - val_binary_accuracy: 0.7302\n",
      "Epoch 11/20\n",
      "479/479 [==============================] - 10s 21ms/step - loss: 0.4589 - binary_accuracy: 0.8112 - val_loss: 0.5460 - val_binary_accuracy: 0.7307\n",
      "Epoch 12/20\n",
      "479/479 [==============================] - 8s 16ms/step - loss: 0.4428 - binary_accuracy: 0.8199 - val_loss: 0.5423 - val_binary_accuracy: 0.7320\n",
      "Epoch 13/20\n",
      "479/479 [==============================] - 10s 21ms/step - loss: 0.4279 - binary_accuracy: 0.8291 - val_loss: 0.5399 - val_binary_accuracy: 0.7328\n",
      "Epoch 14/20\n",
      "479/479 [==============================] - 13s 26ms/step - loss: 0.4141 - binary_accuracy: 0.8362 - val_loss: 0.5386 - val_binary_accuracy: 0.7356\n",
      "Epoch 15/20\n",
      "479/479 [==============================] - 8s 16ms/step - loss: 0.4012 - binary_accuracy: 0.8420 - val_loss: 0.5382 - val_binary_accuracy: 0.7346\n",
      "Epoch 16/20\n",
      "479/479 [==============================] - 10s 21ms/step - loss: 0.3892 - binary_accuracy: 0.8467 - val_loss: 0.5386 - val_binary_accuracy: 0.7375\n",
      "Epoch 17/20\n",
      "479/479 [==============================] - 10s 21ms/step - loss: 0.3778 - binary_accuracy: 0.8530 - val_loss: 0.5397 - val_binary_accuracy: 0.7390\n",
      "Epoch 18/20\n",
      "479/479 [==============================] - 8s 16ms/step - loss: 0.3671 - binary_accuracy: 0.8586 - val_loss: 0.5415 - val_binary_accuracy: 0.7372\n",
      "Epoch 19/20\n",
      "479/479 [==============================] - 21s 44ms/step - loss: 0.3569 - binary_accuracy: 0.8629 - val_loss: 0.5439 - val_binary_accuracy: 0.7377\n",
      "Epoch 20/20\n",
      "479/479 [==============================] - 31s 65ms/step - loss: 0.3473 - binary_accuracy: 0.8673 - val_loss: 0.5468 - val_binary_accuracy: 0.7406\n",
      "1/1 [==============================] - 0s 78ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 18:56:48.175787: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp88xal1dj/model/data/model/assets\n",
      "Evaluate on test data\n",
      "==============\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.5383 - binary_accuracy: 0.7478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/05 18:56:52 INFO mlflow.tracking._tracking_service.client: üèÉ View run GlobalAveragePooling1D at: http://localhost:5000/#/experiments/4/runs/960f98dcbf494daf9085f409ec0e8a80.\n",
      "2025/01/05 18:56:52 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://localhost:5000/#/experiments/4.\n"
     ]
    }
   ],
   "source": [
    "# Create the datasets\n",
    "train_ds, val_ds, test_ds = dl.to_tensorflow_dataset(\n",
    "    col_name,\n",
    "    PATH_PARQUET,\n",
    "    X_train_full,\n",
    "    X_test_full,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    val_split,\n",
    "    batch_size,\n",
    ")\n",
    "# Create the model\n",
    "model = dl.create_tf_model(\n",
    "    max_tokens, seq_length, None, embedding_dim, layers, train_ds\n",
    ")\n",
    "model.get_layer(\"embedding\").trainable = embedding_trainable\n",
    "model.build(input_shape=(None, 1))\n",
    "mlflow.tensorflow.autolog(checkpoint=False, log_models=True)\n",
    "with mlflow.start_run(run_name=run):\n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "        ],\n",
    "    )\n",
    "    mlflow.log_param(\"batch_size_\", batch_size)\n",
    "    mlflow.log_param(\"validation_split_\", val_split)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluate on test data\")\n",
    "    print(\"==============\")\n",
    "    loss, accuracy = model.evaluate(test_ds)\n",
    "    mlflow.log_metric(\"test_loss\", loss)\n",
    "    mlflow.log_metric(\"test_accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Global Max Pooling run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 14:38:37.629832: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 14:38:39.043850: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdirectml.d6f03b303ac3c4f2eeb8ca631688c9757b361310.so\n",
      "2024-11-30 14:38:39.043904: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdxcore.so\n",
      "2024-11-30 14:38:39.048544: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libd3d12.so\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "2024-11-30 14:38:39.648404: I tensorflow/c/logging.cc:34] DirectML device enumeration: found 1 compatible adapters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 314\n",
      "Tensorflow framework: GPU is available\n",
      "(Index(['hour', 'target', 'text', 'tokenizer with lowercase',\n",
      "       'tokenizer with lowercase, handle stripping, and length reduction',\n",
      "       'tokenizer with lowercase and alpha',\n",
      "       'tokenizer with lowercase, alpha and emoji',\n",
      "       'tokenizer with lowercase, alpha, and no stop words',\n",
      "       'tokenizer with lowercase, alpha and emoji, and no stop words'],\n",
      "      dtype='object'), array([2, 0, 1, 3, 4, 5, 6, 7, 8]))\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1596630 entries, 0 to 799999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   text    1596630 non-null  object\n",
      " 1   target  1596630 non-null  int8  \n",
      "dtypes: int8(1), object(1)\n",
      "memory usage: 25.9+ MB\n",
      "None\n",
      "(12772,) (3194,) (12772,) (3194,)\n"
     ]
    }
   ],
   "source": [
    "# Please restart the kernel before running this cell to free up GPU memory\n",
    "%run environnement.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URI of the MLflow server and the name of the experiment\n",
    "experiment = \"nn_scratch_embedding\"\n",
    "run = \"GlobalMaxPooling1D\"\n",
    "\n",
    "# Args for dataset preparation\n",
    "col_name = \"text\"\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "# Args for the model\n",
    "max_tokens = 5000\n",
    "seq_length = 100\n",
    "embedding_dim = 16\n",
    "embedding_trainable = True\n",
    "epochs = 5\n",
    "layers = (tf.keras.layers.GlobalMaxPooling1D(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/hedredo/github/p7/mlflow/689416981458083287', creation_time=1732973943871, experiment_id='689416981458083287', last_update_time=1732973943871, lifecycle_stage='active', name='nn_scratch_embedding', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the tracking URI\n",
    "mlflow.set_tracking_uri(URI)\n",
    "# try to connect to the server\n",
    "try:\n",
    "    mlflow.tracking.get_tracking_uri()\n",
    "except Exception as e:\n",
    "    print(f\"Cannot connect to the server : {URI}. Check the server status.\")\n",
    "    raise e\n",
    "# Set, and create if necessary, the experiment\n",
    "try:\n",
    "    mlflow.create_experiment(experiment)\n",
    "except:\n",
    "    pass\n",
    "mlflow.set_experiment(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 14:44:00.393956: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  5000\n",
      "Epoch 1/5\n",
      "  5/320 [..............................] - ETA: 4s - loss: 0.6959 - binary_accuracy: 0.4813  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0049s vs `on_train_batch_end` time: 0.0088s). Check your callbacks.\n",
      "  9/320 [..............................] - ETA: 4s - loss: 0.6928 - binary_accuracy: 0.5104"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 14:44:00.205966: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-30 14:44:00.254594: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-30 14:44:00.254653: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-30 14:44:00.256125: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-30 14:44:00.256161: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-30 14:44:00.257898: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-30 14:44:00.257941: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/320 [============================>.] - ETA: 0s - loss: 0.6766 - binary_accuracy: 0.6099"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 14:44:04.804892: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-30 14:44:04.826158: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-30 14:44:04.826204: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 5s 15ms/step - loss: 0.6763 - binary_accuracy: 0.6107 - val_loss: 0.6489 - val_binary_accuracy: 0.7092\n",
      "Epoch 2/5\n",
      "320/320 [==============================] - 4s 14ms/step - loss: 0.6065 - binary_accuracy: 0.7505 - val_loss: 0.5759 - val_binary_accuracy: 0.7299\n",
      "Epoch 3/5\n",
      "320/320 [==============================] - 4s 14ms/step - loss: 0.5284 - binary_accuracy: 0.7782 - val_loss: 0.5336 - val_binary_accuracy: 0.7386\n",
      "Epoch 4/5\n",
      "320/320 [==============================] - 4s 14ms/step - loss: 0.4698 - binary_accuracy: 0.8064 - val_loss: 0.5142 - val_binary_accuracy: 0.7476\n",
      "Epoch 5/5\n",
      "320/320 [==============================] - 4s 14ms/step - loss: 0.4235 - binary_accuracy: 0.8261 - val_loss: 0.5055 - val_binary_accuracy: 0.7503\n",
      "1/1 [==============================] - 0s 53ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 14:44:22.927136: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp9e166qd6/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/30 14:44:25 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp9e166qd6/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "==============\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.4955 - binary_accuracy: 0.7539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/30 14:44:26 INFO mlflow.tracking._tracking_service.client: üèÉ View run GlobalMaxPooling1D at: http://localhost:5000/#/experiments/689416981458083287/runs/b5efda2cee954ff1a88923f357bc0525.\n",
      "2024/11/30 14:44:26 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://localhost:5000/#/experiments/689416981458083287.\n"
     ]
    }
   ],
   "source": [
    "# Create the datasets\n",
    "train_ds, val_ds, test_ds = to_tensorflow_dataset(\n",
    "    col_name, PATH_PARQUET, X_train, X_test, y_train, y_test, val_split, batch_size\n",
    ")\n",
    "# Create the model\n",
    "model = create_tf_model(\n",
    "    max_tokens, seq_length, custom_standardization, embedding_dim, layers, train_ds\n",
    ")\n",
    "model.get_layer(\"embedding\").trainable = embedding_trainable\n",
    "model.build(input_shape=(None, 1))\n",
    "mlflow.tensorflow.autolog(checkpoint=False, log_datasets=False, log_models=True)\n",
    "with mlflow.start_run(run_name=run):\n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "    )\n",
    "    mlflow.log_param(\"batch_size_\", batch_size)\n",
    "    mlflow.log_param(\"validation_split_\", val_split)\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluate on test data\")\n",
    "    print(\"==============\")\n",
    "    loss, accuracy = model.evaluate(test_ds)\n",
    "    mlflow.log_param(\"dataset_training_rows\", X_train.shape[0])\n",
    "    mlflow.log_param(\n",
    "        \"dataset_nb_features\", X_train.shape[1] if len(X_train.shape) > 1 else 1\n",
    "    )\n",
    "    mlflow.log_param(\"dataset_schema\", str(X_train.dtypes))\n",
    "    mlflow.log_metric(\"test_loss\", loss)\n",
    "    mlflow.log_metric(\"test_accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GLOVE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gloval Average Pooling Run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:14:55.072454: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-22 19:14:56.631337: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdirectml.d6f03b303ac3c4f2eeb8ca631688c9757b361310.so\n",
      "2024-11-22 19:14:56.631397: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdxcore.so\n",
      "2024-11-22 19:14:56.636721: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libd3d12.so\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "2024-11-22 19:14:57.231003: I tensorflow/c/logging.cc:34] DirectML device enumeration: found 1 compatible adapters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 314\n",
      "Tensorflow framework: GPU is available\n",
      "(Index(['hour', 'target', 'text', 'tokenizer with lowercase',\n",
      "       'tokenizer with lowercase, handle stripping, and length reduction',\n",
      "       'tokenizer with lowercase and alpha',\n",
      "       'tokenizer with lowercase, alpha and emoji',\n",
      "       'tokenizer with lowercase, alpha, and no stop words',\n",
      "       'tokenizer with lowercase, alpha and emoji, and no stop words'],\n",
      "      dtype='object'), array([2, 0, 1, 3, 4, 5, 6, 7, 8]))\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1596630 entries, 0 to 799999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   text    1596630 non-null  object\n",
      " 1   target  1596630 non-null  int8  \n",
      "dtypes: int8(1), object(1)\n",
      "memory usage: 25.9+ MB\n",
      "None\n",
      "(12772,) (3194,) (12772,) (3194,)\n"
     ]
    }
   ],
   "source": [
    "# Please restart the kernel before running this cell to free GPU memory\n",
    "%run environnement.ipynb\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the glove-twitter-100 model\n",
    "repo_id = \"fse/glove-twitter-100\"\n",
    "model_file = hf_hub_download(repo_id=repo_id, filename=\"glove-twitter-100.model\")\n",
    "vector_file = hf_hub_download(\n",
    "    repo_id=repo_id, filename=\"glove-twitter-100.model.vectors.npy\"\n",
    ")\n",
    "glove = KeyedVectors.load(model_file, mmap=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the name of your experiment\n",
    "experiment = \"nn_glove_embeddings\"\n",
    "run = \"GlobalAveragePooling1D\"\n",
    "# Args for dataset preparation\n",
    "col_name = \"text\"\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "# Args for the model\n",
    "max_tokens = 5000\n",
    "seq_length = 100\n",
    "embedding_dim = 100\n",
    "embedding_trainable = False\n",
    "epochs = 3\n",
    "layers = (tf.keras.layers.GlobalAveragePooling1D(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/246698213555700691', creation_time=1732299334237, experiment_id='246698213555700691', last_update_time=1732299334237, lifecycle_stage='active', name='nn_glove_embeddings', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the tracking URI\n",
    "mlflow.set_tracking_uri(URI)\n",
    "# try to connect to the server\n",
    "try:\n",
    "    mlflow.tracking.get_tracking_uri()\n",
    "except Exception as e:\n",
    "    print(f\"Cannot connect to the server : {URI}. Check the server status.\")\n",
    "    raise e\n",
    "# Set, and create if necessary, the experiment\n",
    "try:\n",
    "    mlflow.create_experiment(experiment)\n",
    "except:\n",
    "    pass\n",
    "mlflow.set_experiment(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:15:43.453711: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-22 19:15:43.455013: I tensorflow/c/logging.cc:34] DirectML: creating device on adapter 0 (AMD Radeon RX 6700 XT)\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "2024-11-22 19:15:43.527045: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:15:43.527077: W tensorflow/core/common_runtime/pluggable_device/pluggable_device_bfc_allocator.cc:37] Ignoring the value of TF_FORCE_GPU_ALLOW_GROWTH because force_memory_growth was requested by the device.\n",
      "2024-11-22 19:15:43.527100: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-22 19:15:43.649039: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/22 19:15:44 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'ascii' codec can't decode byte 0xef in position 31: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:15:45.006199: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-22 19:15:45.064555: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:15:45.064633: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-22 19:15:45.066950: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:15:45.067014: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-22 19:15:45.105240: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:15:45.105353: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/320 [..............................] - ETA: 16s - loss: 0.6986 - binary_accuracy: 0.5188WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0058s vs `on_train_batch_end` time: 0.0524s). Check your callbacks.\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.6867 - binary_accuracy: 0.5490"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:15:58.370998: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-22 19:15:58.393988: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:15:58.394042: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 14s 51ms/step - loss: 0.6867 - binary_accuracy: 0.5488 - val_loss: 0.6816 - val_binary_accuracy: 0.5444\n",
      "Epoch 2/3\n",
      "320/320 [==============================] - 13s 42ms/step - loss: 0.6725 - binary_accuracy: 0.6333 - val_loss: 0.6694 - val_binary_accuracy: 0.5855\n",
      "Epoch 3/3\n",
      "320/320 [==============================] - 17s 52ms/step - loss: 0.6569 - binary_accuracy: 0.6675 - val_loss: 0.6549 - val_binary_accuracy: 0.6168\n",
      "1/1 [==============================] - 0s 70ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:16:28.861709: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpc6cww1vm/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/22 19:16:32 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpc6cww1vm/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "==============\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.6554 - binary_accuracy: 0.6162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/22 19:16:33 INFO mlflow.tracking._tracking_service.client: üèÉ View run GlobalAveragePooling1D at: http://localhost:5000/#/experiments/246698213555700691/runs/8c97a6124a964b228811579944c9f76e.\n",
      "2024/11/22 19:16:33 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://localhost:5000/#/experiments/246698213555700691.\n"
     ]
    }
   ],
   "source": [
    "# Create the datasets\n",
    "train_ds, val_ds, test_ds = to_tensorflow_dataset(\n",
    "    col_name, PATH_PARQUET, X_train, X_test, y_train, y_test, val_split, batch_size\n",
    ")\n",
    "\n",
    "# Create the model\n",
    "model = create_tf_model(\n",
    "    max_tokens, seq_length, embedding_dim, layers, train_ds, pretrained_weights=glove\n",
    ")\n",
    "# set the embedding layer trainable or not\n",
    "model.get_layer(\"embedding\").trainable = embedding_trainable\n",
    "mlflow.tensorflow.autolog(checkpoint=False, log_models=True)\n",
    "with mlflow.start_run(run_name=run):\n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "    )\n",
    "    mlflow.log_param(\"batch_size_\", batch_size)\n",
    "    mlflow.log_param(\"validation_split_\", val_split)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluate on test data\")\n",
    "    print(\"==============\")\n",
    "    loss, accuracy = model.evaluate(test_ds)\n",
    "    mlflow.log_metric(\"test_loss\", loss)\n",
    "    mlflow.log_metric(\"test_accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Global Max Pooling Run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:16:54.391837: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-22 19:16:55.233833: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdirectml.d6f03b303ac3c4f2eeb8ca631688c9757b361310.so\n",
      "2024-11-22 19:16:55.233903: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdxcore.so\n",
      "2024-11-22 19:16:55.238588: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libd3d12.so\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "2024-11-22 19:16:55.404099: I tensorflow/c/logging.cc:34] DirectML device enumeration: found 1 compatible adapters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 314\n",
      "Tensorflow framework: GPU is available\n",
      "(Index(['hour', 'target', 'text', 'tokenizer with lowercase',\n",
      "       'tokenizer with lowercase, handle stripping, and length reduction',\n",
      "       'tokenizer with lowercase and alpha',\n",
      "       'tokenizer with lowercase, alpha and emoji',\n",
      "       'tokenizer with lowercase, alpha, and no stop words',\n",
      "       'tokenizer with lowercase, alpha and emoji, and no stop words'],\n",
      "      dtype='object'), array([2, 0, 1, 3, 4, 5, 6, 7, 8]))\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1596630 entries, 0 to 799999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   text    1596630 non-null  object\n",
      " 1   target  1596630 non-null  int8  \n",
      "dtypes: int8(1), object(1)\n",
      "memory usage: 25.9+ MB\n",
      "None\n",
      "(12772,) (3194,) (12772,) (3194,)\n"
     ]
    }
   ],
   "source": [
    "# Please restart the kernel before running this cell to free GPU memory\n",
    "%run environnement.ipynb\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the glove-twitter-100 model\n",
    "repo_id = \"fse/glove-twitter-100\"\n",
    "model_file = hf_hub_download(repo_id=repo_id, filename=\"glove-twitter-100.model\")\n",
    "vector_file = hf_hub_download(\n",
    "    repo_id=repo_id, filename=\"glove-twitter-100.model.vectors.npy\"\n",
    ")\n",
    "glove = KeyedVectors.load(model_file, mmap=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the name of your experiment\n",
    "experiment = \"nn_glove_embeddings\"\n",
    "run = \"GlobalMaxPooling1D\"\n",
    "# Args for dataset preparation\n",
    "col_name = \"text\"\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "# Args for the model\n",
    "max_tokens = 5000\n",
    "seq_length = 100\n",
    "embedding_dim = 100\n",
    "embedding_trainable = False\n",
    "epochs = 3\n",
    "layers = (tf.keras.layers.GlobalMaxPooling1D(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/246698213555700691', creation_time=1732299334237, experiment_id='246698213555700691', last_update_time=1732299334237, lifecycle_stage='active', name='nn_glove_embeddings', tags={}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the tracking URI\n",
    "mlflow.set_tracking_uri(URI)\n",
    "# try to connect to the server\n",
    "try:\n",
    "    mlflow.tracking.get_tracking_uri()\n",
    "except Exception as e:\n",
    "    print(f\"Cannot connect to the server : {URI}. Check the server status.\")\n",
    "    raise e\n",
    "# Set, and create if necessary, the experiment\n",
    "try:\n",
    "    mlflow.create_experiment(experiment)\n",
    "except:\n",
    "    pass\n",
    "mlflow.set_experiment(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:17:20.117409: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-22 19:17:20.118892: I tensorflow/c/logging.cc:34] DirectML: creating device on adapter 0 (AMD Radeon RX 6700 XT)\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "2024-11-22 19:17:20.180930: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:17:20.180959: W tensorflow/core/common_runtime/pluggable_device/pluggable_device_bfc_allocator.cc:37] Ignoring the value of TF_FORCE_GPU_ALLOW_GROWTH because force_memory_growth was requested by the device.\n",
      "2024-11-22 19:17:20.180983: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-22 19:17:20.313160: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/22 19:17:21 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'ascii' codec can't decode byte 0xef in position 31: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:17:21.667546: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-22 19:17:21.729626: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:17:21.729674: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-22 19:17:21.731446: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:17:21.731484: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-22 19:17:21.734338: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:17:21.734386: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/320 [..............................] - ETA: 16s - loss: 0.7132 - binary_accuracy: 0.4437WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0058s vs `on_train_batch_end` time: 0.0513s). Check your callbacks.\n",
      "319/320 [============================>.] - ETA: 0s - loss: 0.6794 - binary_accuracy: 0.5635"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:17:37.965533: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-22 19:17:37.986932: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:17:37.986977: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 17s 51ms/step - loss: 0.6794 - binary_accuracy: 0.5634 - val_loss: 0.6591 - val_binary_accuracy: 0.6231\n",
      "Epoch 2/3\n",
      "320/320 [==============================] - 13s 42ms/step - loss: 0.6315 - binary_accuracy: 0.6742 - val_loss: 0.6223 - val_binary_accuracy: 0.6755\n",
      "Epoch 3/3\n",
      "320/320 [==============================] - 17s 52ms/step - loss: 0.5823 - binary_accuracy: 0.7303 - val_loss: 0.5849 - val_binary_accuracy: 0.7115\n",
      "1/1 [==============================] - 0s 70ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:18:08.437812: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp1ci9kodr/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/22 19:18:11 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp1ci9kodr/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "==============\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.5891 - binary_accuracy: 0.7054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/22 19:18:12 INFO mlflow.tracking._tracking_service.client: üèÉ View run GlobalMaxPooling1D at: http://localhost:5000/#/experiments/246698213555700691/runs/a4d4aa4b49994aff9aad8c6927d9292f.\n",
      "2024/11/22 19:18:12 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://localhost:5000/#/experiments/246698213555700691.\n"
     ]
    }
   ],
   "source": [
    "# Create the datasets\n",
    "train_ds, val_ds, test_ds = to_tensorflow_dataset(\n",
    "    col_name, PATH_PARQUET, X_train, X_test, y_train, y_test, val_split, batch_size\n",
    ")\n",
    "\n",
    "# Create the model\n",
    "model = create_tf_model(\n",
    "    max_tokens, seq_length, embedding_dim, layers, train_ds, pretrained_weights=glove\n",
    ")\n",
    "# set the embedding layer trainable or not\n",
    "model.get_layer(\"embedding\").trainable = embedding_trainable\n",
    "mlflow.tensorflow.autolog(checkpoint=False, log_models=True)\n",
    "with mlflow.start_run(run_name=run):\n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "    )\n",
    "    mlflow.log_param(\"batch_size_\", batch_size)\n",
    "    mlflow.log_param(\"validation_split_\", val_split)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluate on test data\")\n",
    "    print(\"==============\")\n",
    "    loss, accuracy = model.evaluate(test_ds)\n",
    "    mlflow.log_metric(\"test_loss\", loss)\n",
    "    mlflow.log_metric(\"test_accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FASTTEXT EMBEDDINGS + CUSTOM NN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Global Average Pooling Run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:19:04.747451: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-22 19:19:05.618176: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdirectml.d6f03b303ac3c4f2eeb8ca631688c9757b361310.so\n",
      "2024-11-22 19:19:05.618239: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdxcore.so\n",
      "2024-11-22 19:19:05.623202: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libd3d12.so\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "2024-11-22 19:19:05.789649: I tensorflow/c/logging.cc:34] DirectML device enumeration: found 1 compatible adapters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 314\n",
      "Tensorflow framework: GPU is available\n",
      "(Index(['hour', 'target', 'text', 'tokenizer with lowercase',\n",
      "       'tokenizer with lowercase, handle stripping, and length reduction',\n",
      "       'tokenizer with lowercase and alpha',\n",
      "       'tokenizer with lowercase, alpha and emoji',\n",
      "       'tokenizer with lowercase, alpha, and no stop words',\n",
      "       'tokenizer with lowercase, alpha and emoji, and no stop words'],\n",
      "      dtype='object'), array([2, 0, 1, 3, 4, 5, 6, 7, 8]))\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1596630 entries, 0 to 799999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   text    1596630 non-null  object\n",
      " 1   target  1596630 non-null  int8  \n",
      "dtypes: int8(1), object(1)\n",
      "memory usage: 25.9+ MB\n",
      "None\n",
      "(12772,) (3194,) (12772,) (3194,)\n"
     ]
    }
   ],
   "source": [
    "# Please restart the kernel before running this cell to free GPU memory\n",
    "%run environnement.ipynb\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fasttext embeddings trained on twitter data\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"facebook/fasttext-en-vectors\", filename=\"model.bin\"\n",
    ")\n",
    "fastxt = fasttext.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the name of your experiment\n",
    "experiment = \"nn_fasttext_embeddings\"\n",
    "run = \"GlobalAveragePooling1D\"\n",
    "\n",
    "# Args for dataset preparation\n",
    "col_name = \"text\"\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "# Args for the model\n",
    "max_tokens = 5000\n",
    "seq_length = 100\n",
    "embedding_dim = 300\n",
    "embedding_trainable = False\n",
    "epochs = 3\n",
    "layers = (tf.keras.layers.GlobalAveragePooling1D(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:19:14.117706: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-22 19:19:14.121834: I tensorflow/c/logging.cc:34] DirectML: creating device on adapter 0 (AMD Radeon RX 6700 XT)\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "2024-11-22 19:19:14.926604: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:19:14.926815: W tensorflow/core/common_runtime/pluggable_device/pluggable_device_bfc_allocator.cc:37] Ignoring the value of TF_FORCE_GPU_ALLOW_GROWTH because force_memory_growth was requested by the device.\n",
      "2024-11-22 19:19:14.927028: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-22 19:19:15.665452: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/22 19:19:17 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'ascii' codec can't decode byte 0xef in position 31: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:19:18.494021: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-22 19:19:18.629321: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:19:18.629378: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-22 19:19:18.632752: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:19:18.632809: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-22 19:19:18.638416: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:19:18.638491: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/320 [..............................] - ETA: 45s - loss: 0.6932 - binary_accuracy: 0.4740WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0138s vs `on_train_batch_end` time: 0.1437s). Check your callbacks.\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.6772 - binary_accuracy: 0.5931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:20:00.362353: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-22 19:20:00.385027: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:20:00.385072: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 43s 129ms/step - loss: 0.6772 - binary_accuracy: 0.5931 - val_loss: 0.6510 - val_binary_accuracy: 0.6172\n",
      "Epoch 2/3\n",
      "320/320 [==============================] - 37s 116ms/step - loss: 0.6034 - binary_accuracy: 0.7026 - val_loss: 0.5773 - val_binary_accuracy: 0.7045\n",
      "Epoch 3/3\n",
      "320/320 [==============================] - 140s 437ms/step - loss: 0.5263 - binary_accuracy: 0.7613 - val_loss: 0.5368 - val_binary_accuracy: 0.7382\n",
      "1/1 [==============================] - 0s 70ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:22:57.383693: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpr1gxd5sp/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/22 19:23:01 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpr1gxd5sp/model, flavor: tensorflow). Fall back to return ['tensorflow==2.10.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "==============\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 0.5398 - binary_accuracy: 0.7448\n"
     ]
    }
   ],
   "source": [
    "# Create the datasets\n",
    "train_ds, val_ds, test_ds = to_tensorflow_dataset(\n",
    "    col_name, PATH_PARQUET, X_train, X_test, y_train, y_test, val_split, batch_size\n",
    ")\n",
    "# Create the model\n",
    "model = create_tf_model(\n",
    "    max_tokens, seq_length, embedding_dim, layers, train_ds, pretrained_weights=fastxt\n",
    ")\n",
    "# set the embedding layer trainable or not\n",
    "model.get_layer(\"embedding\").trainable = embedding_trainable\n",
    "mlflow.tensorflow.autolog(checkpoint=False, log_models=True)\n",
    "with mlflow.start_run(run_name=run):\n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "    )\n",
    "    mlflow.log_param(\"batch_size_\", batch_size)\n",
    "    mlflow.log_param(\"validation_split_\", val_split)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluate on test data\")\n",
    "    print(\"==============\")\n",
    "    loss, accuracy = model.evaluate(test_ds)\n",
    "    mlflow.log_metric(\"test_loss\", loss)\n",
    "    mlflow.log_metric(\"test_accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Global Max Pooling Run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:19:04.747451: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-22 19:19:05.618176: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdirectml.d6f03b303ac3c4f2eeb8ca631688c9757b361310.so\n",
      "2024-11-22 19:19:05.618239: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdxcore.so\n",
      "2024-11-22 19:19:05.623202: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libd3d12.so\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "2024-11-22 19:19:05.789649: I tensorflow/c/logging.cc:34] DirectML device enumeration: found 1 compatible adapters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 314\n",
      "Tensorflow framework: GPU is available\n",
      "(Index(['hour', 'target', 'text', 'tokenizer with lowercase',\n",
      "       'tokenizer with lowercase, handle stripping, and length reduction',\n",
      "       'tokenizer with lowercase and alpha',\n",
      "       'tokenizer with lowercase, alpha and emoji',\n",
      "       'tokenizer with lowercase, alpha, and no stop words',\n",
      "       'tokenizer with lowercase, alpha and emoji, and no stop words'],\n",
      "      dtype='object'), array([2, 0, 1, 3, 4, 5, 6, 7, 8]))\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1596630 entries, 0 to 799999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   text    1596630 non-null  object\n",
      " 1   target  1596630 non-null  int8  \n",
      "dtypes: int8(1), object(1)\n",
      "memory usage: 25.9+ MB\n",
      "None\n",
      "(12772,) (3194,) (12772,) (3194,)\n"
     ]
    }
   ],
   "source": [
    "# Please restart the kernel before running this cell to free GPU memory\n",
    "%run environnement.ipynb\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fasttext embeddings trained on twitter data\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"facebook/fasttext-en-vectors\", filename=\"model.bin\"\n",
    ")\n",
    "fastxt = fasttext.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the name of your experiment\n",
    "experiment = \"nn_fasttext_embeddings\"\n",
    "run = \"GlobalMaxPooling1D\"\n",
    "\n",
    "# Args for dataset preparation\n",
    "col_name = \"text\"\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "# Args for the model\n",
    "max_tokens = 5000\n",
    "seq_length = 100\n",
    "embedding_dim = 300\n",
    "embedding_trainable = False\n",
    "epochs = 3\n",
    "layers = (tf.keras.layers.GlobalMaxPooling1D(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:19:14.117706: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-22 19:19:14.121834: I tensorflow/c/logging.cc:34] DirectML: creating device on adapter 0 (AMD Radeon RX 6700 XT)\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "2024-11-22 19:19:14.926604: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:19:14.926815: W tensorflow/core/common_runtime/pluggable_device/pluggable_device_bfc_allocator.cc:37] Ignoring the value of TF_FORCE_GPU_ALLOW_GROWTH because force_memory_growth was requested by the device.\n",
      "2024-11-22 19:19:14.927028: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-22 19:19:15.665452: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/22 19:19:17 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'ascii' codec can't decode byte 0xef in position 31: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 19:19:18.494021: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-22 19:19:18.629321: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:19:18.629378: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-22 19:19:18.632752: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:19:18.632809: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-22 19:19:18.638416: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-22 19:19:18.638491: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/320 [..............................] - ETA: 45s - loss: 0.6932 - binary_accuracy: 0.4740WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0138s vs `on_train_batch_end` time: 0.1437s). Check your callbacks.\n",
      "320/320 [==============================] - ETA: 0s - loss: 0.6772 - binary_accuracy: 0.5931"
     ]
    }
   ],
   "source": [
    "# Create the datasets\n",
    "train_ds, val_ds, test_ds = to_tensorflow_dataset(\n",
    "    col_name, PATH_PARQUET, X_train, X_test, y_train, y_test, val_split, batch_size\n",
    ")\n",
    "# Create the model\n",
    "model = create_tf_model(\n",
    "    max_tokens, seq_length, embedding_dim, layers, train_ds, pretrained_weights=fastxt\n",
    ")\n",
    "# set the embedding layer trainable or not\n",
    "model.get_layer(\"embedding\").trainable = embedding_trainable\n",
    "mlflow.tensorflow.autolog(checkpoint=False, log_models=True)\n",
    "with mlflow.start_run(run_name=run):\n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "    )\n",
    "    mlflow.log_param(\"batch_size_\", batch_size)\n",
    "    mlflow.log_param(\"validation_split_\", val_split)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluate on test data\")\n",
    "    print(\"==============\")\n",
    "    loss, accuracy = model.evaluate(test_ds)\n",
    "    mlflow.log_metric(\"test_loss\", loss)\n",
    "    mlflow.log_metric(\"test_accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIDERECTIONAL LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 18:39:58.435638: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-22 18:39:59.344145: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdirectml.d6f03b303ac3c4f2eeb8ca631688c9757b361310.so\n",
      "2024-11-22 18:39:59.344209: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libdxcore.so\n",
      "2024-11-22 18:39:59.349589: I tensorflow/c/logging.cc:34] Successfully opened dynamic library libd3d12.so\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "Dropped Escape call with ulEscapeCode : 0x03007703\n",
      "2024-11-22 18:39:59.516451: I tensorflow/c/logging.cc:34] DirectML device enumeration: found 1 compatible adapters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 314\n",
      "Tensorflow framework: GPU is available\n",
      "(Index(['hour', 'target', 'text', 'tokenizer with lowercase',\n",
      "       'tokenizer with lowercase, handle stripping, and length reduction',\n",
      "       'tokenizer with lowercase and alpha',\n",
      "       'tokenizer with lowercase, alpha and emoji',\n",
      "       'tokenizer with lowercase, alpha, and no stop words',\n",
      "       'tokenizer with lowercase, alpha and emoji, and no stop words'],\n",
      "      dtype='object'), array([2, 0, 1, 3, 4, 5, 6, 7, 8]))\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1596630 entries, 0 to 799999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   text    1596630 non-null  object\n",
      " 1   target  1596630 non-null  int8  \n",
      "dtypes: int8(1), object(1)\n",
      "memory usage: 25.9+ MB\n",
      "None\n",
      "(12772,) (3194,) (12772,) (3194,)\n"
     ]
    }
   ],
   "source": [
    "# Please restart the kernel before running this cell to free GPU memory\n",
    "%run environnement.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/846190965426187584', creation_time=1731654957941, experiment_id='846190965426187584', last_update_time=1731654957941, lifecycle_stage='active', name='neural_network_SEQ_embedding', tags={}>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the name of your experiment\n",
    "experiment = \"nn_bidirectionnalLSTM_embedding\"\n",
    "\n",
    "# Set the tracking URI\n",
    "mlflow.set_tracking_uri(URI)\n",
    "# try to connect to the server\n",
    "try:\n",
    "    mlflow.tracking.get_tracking_uri()\n",
    "except Exception as e:\n",
    "    print(f\"Cannot connect to the server : {URI}. Check the server status.\")\n",
    "    raise e\n",
    "# Set, and create if necessary, the experiment\n",
    "try:\n",
    "    mlflow.create_experiment(experiment)\n",
    "except:\n",
    "    pass\n",
    "mlflow.set_experiment(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args for dataset preparation\n",
    "col_name = \"text\"\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "# Create the datasets\n",
    "train_ds, val_ds, test_ds = prepare_tf_dataset(col_name, val_split, batch_size)\n",
    "\n",
    "# Args for the model\n",
    "max_tokens = 1000\n",
    "seq_length = 100\n",
    "embedding_dim = 16\n",
    "embedding_trainable = True\n",
    "epochs = 15\n",
    "additionnal_layers = [\n",
    "    (\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8)),\n",
    "    ),\n",
    "]\n",
    "runs = (\"BiderectionalLSTM\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 19:59:42.045555: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/15 19:59:48 WARNING mlflow.tensorflow: Failed to log training dataset information to MLflow Tracking. Reason: 'ascii' codec can't decode byte 0xef in position 63: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 19:59:50.504612: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-15 19:59:50.831360: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-15 19:59:50.831405: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-15 19:59:50.832566: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-15 19:59:50.832599: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-15 19:59:50.833658: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-15 19:59:50.833700: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-15 19:59:50.834875: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-15 19:59:50.834909: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-15 19:59:50.835941: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-15 19:59:50.835974: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-15 19:59:50.836966: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-15 19:59:50.836999: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-15 19:59:50.838824: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-15 19:59:50.838858: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25405 MB memory) -> physical PluggableDevice (device: 0, name: DML, pci bus id: <undefined>)\n",
      "2024-11-15 19:59:50.844760: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at partitioned_function_ops.cc:115 : INVALID_ARGUMENT: No OpKernel was registered to support Op 'CudnnRNN' used by {{node CudnnRNN}} with these attrs: [T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", seed2=0, is_training=true, dropout=0, seed=0]\n",
      "Registered devices: [CPU, GPU]\n",
      "Registered kernels:\n",
      "  <no registered kernels>\n",
      "\n",
      "\t [[CudnnRNN]]\n",
      "2024-11-15 19:59:50.846345: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at partitioned_function_ops.cc:115 : INVALID_ARGUMENT: No OpKernel was registered to support Op 'CudnnRNN' used by {{node CudnnRNN}} with these attrs: [rnn_mode=\"lstm\", is_training=true, seed2=0, seed=0, dropout=0, input_mode=\"linear_input\", T=DT_FLOAT, direction=\"unidirectional\"]\n",
      "Registered devices: [CPU, GPU]\n",
      "Registered kernels:\n",
      "  <no registered kernels>\n",
      "\n",
      "\t [[CudnnRNN]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nNo OpKernel was registered to support Op 'CudnnRNN' used by {{node CudnnRNN}} with these attrs: [T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", seed2=0, is_training=true, dropout=0, seed=0]\nRegistered devices: [CPU, GPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[CudnnRNN]]\n\t [[sequential_2/bidirectional_2/forward_lstm_2/PartitionedCall]] [Op:__inference_train_function_55753]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n",
      "\u001b[1;32m      5\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mtensorflow\u001b[38;5;241m.\u001b[39mautolog(checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, log_models\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run(run_name\u001b[38;5;241m=\u001b[39mrun_name):\n",
      "\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n",
      "\u001b[0;32m----> 8\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m      9\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     10\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     11\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     12\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     14\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size_\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size)\n",
      "\u001b[1;32m     15\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_split_\u001b[39m\u001b[38;5;124m\"\u001b[39m,val_split)\n",
      "\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:578\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    568\u001b[0m try_log_autologging_event(\n",
      "\u001b[1;32m    569\u001b[0m     AutologgingEventLogger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mlog_patch_function_start,\n",
      "\u001b[1;32m    570\u001b[0m     session,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    574\u001b[0m     kwargs,\n",
      "\u001b[1;32m    575\u001b[0m )\n",
      "\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m patch_is_class:\n",
      "\u001b[0;32m--> 578\u001b[0m     \u001b[43mpatch_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    580\u001b[0m     patch_function(call_original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:165\u001b[0m, in \u001b[0;36mPatchFunction.call\u001b[0;34m(cls, original, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    163\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n",
      "\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mcls\u001b[39m, original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:176\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_exception(e)\n",
      "\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# Regardless of what happens during the `_on_exception` callback, reraise\u001b[39;00m\n",
      "\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# the original implementation exception once the callback completes\u001b[39;00m\n",
      "\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:169\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_patch_implementation\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m    171\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:227\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.PatchWithManagedRun._patch_implementation\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mactive_run():\n",
      "\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanaged_run \u001b[38;5;241m=\u001b[39m create_managed_run()\n",
      "\u001b[0;32m--> 227\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_patch_implementation\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanaged_run:\n",
      "\u001b[1;32m    230\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mend_run(RunStatus\u001b[38;5;241m.\u001b[39mto_string(RunStatus\u001b[38;5;241m.\u001b[39mFINISHED))\n",
      "\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/mlflow/tensorflow/__init__.py:1353\u001b[0m, in \u001b[0;36mautolog.<locals>.FitPatch._patch_implementation\u001b[0;34m(self, original, inst, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m   1347\u001b[0m         _logger\u001b[38;5;241m.\u001b[39mwarning(\n",
      "\u001b[1;32m   1348\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to log training dataset information to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   1349\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLflow Tracking. Reason: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m   1350\u001b[0m             e,\n",
      "\u001b[1;32m   1351\u001b[0m         )\n",
      "\u001b[0;32m-> 1353\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43moriginal\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_models:\n",
      "\u001b[1;32m   1356\u001b[0m     _log_keras_model(history, args)\n",
      "\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:561\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[0;34m(*og_args, **og_kwargs)\u001b[0m\n",
      "\u001b[1;32m    558\u001b[0m         original_result \u001b[38;5;241m=\u001b[39m original(\u001b[38;5;241m*\u001b[39m_og_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_og_kwargs)\n",
      "\u001b[1;32m    559\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n",
      "\u001b[0;32m--> 561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_original_fn_with_event_logging\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_original_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:496\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[0;34m(original_fn, og_args, og_kwargs)\u001b[0m\n",
      "\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m    488\u001b[0m     try_log_autologging_event(\n",
      "\u001b[1;32m    489\u001b[0m         AutologgingEventLogger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mlog_original_function_start,\n",
      "\u001b[1;32m    490\u001b[0m         session,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    494\u001b[0m         og_kwargs,\n",
      "\u001b[1;32m    495\u001b[0m     )\n",
      "\u001b[0;32m--> 496\u001b[0m     original_fn_result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    498\u001b[0m     try_log_autologging_event(\n",
      "\u001b[1;32m    499\u001b[0m         AutologgingEventLogger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mlog_original_function_success,\n",
      "\u001b[1;32m    500\u001b[0m         session,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    504\u001b[0m         og_kwargs,\n",
      "\u001b[1;32m    505\u001b[0m     )\n",
      "\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_fn_result\n",
      "\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:558\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[0;34m(*_og_args, **_og_kwargs)\u001b[0m\n",
      "\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n",
      "\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# during original function execution, even if silent mode is enabled\u001b[39;00m\n",
      "\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n",
      "\u001b[1;32m    553\u001b[0m \u001b[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n",
      "\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n",
      "\u001b[1;32m    555\u001b[0m     disable_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "\u001b[1;32m    556\u001b[0m     reroute_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "\u001b[1;32m    557\u001b[0m ):\n",
      "\u001b[0;32m--> 558\u001b[0m     original_result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_og_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_og_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n",
      "\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n",
      "\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n",
      "\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\n",
      "File \u001b[0;32m~/miniconda310/envs/p7/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n",
      "\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n",
      "\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n",
      "\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n",
      "\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n",
      "\n",
      "No OpKernel was registered to support Op 'CudnnRNN' used by {{node CudnnRNN}} with these attrs: [T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", seed2=0, is_training=true, dropout=0, seed=0]\n",
      "Registered devices: [CPU, GPU]\n",
      "Registered kernels:\n",
      "  <no registered kernels>\n",
      "\n",
      "\t [[CudnnRNN]]\n",
      "\t [[sequential_2/bidirectional_2/forward_lstm_2/PartitionedCall]] [Op:__inference_train_function_55753]"
     ]
    }
   ],
   "source": [
    "for layers, run_name in zip(additionnal_layers, runs):\n",
    "    # Create the model\n",
    "    model = create_tf_model(max_tokens, seq_length, embedding_dim, layers)\n",
    "    model.get_layer(\"embedding\").trainable = embedding_trainable\n",
    "    mlflow.tensorflow.autolog(checkpoint=False, log_models=True)\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Fit the model\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "        )\n",
    "        mlflow.log_param(\"batch_size_\", batch_size)\n",
    "        mlflow.log_param(\"validation_split_\", val_split)\n",
    "\n",
    "        # Evaluate the model\n",
    "        print(\"Evaluate on test data\")\n",
    "        print(\"==============\")\n",
    "        loss, accuracy = model.evaluate(test_ds)\n",
    "        mlflow.log_metric(\"test_loss\", loss)\n",
    "        mlflow.log_metric(\"test_accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTENCE TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c12c17d5f2f42bfbd3ff6ddcd393ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the x_train data aligned on cleaned text corpus\n",
    "X_train, X_test, y_train, y_test = load_splits_from_parquet(\n",
    "    X_train, X_test, y_train, y_test, col_name=\"tokenizer with lowercase\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "X_train_encoded = model.encode(X_train.values, device=\"cpu\")\n",
    "X_test_encoded = model.encode(X_test.values, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.75      1601\n",
      "           1       0.75      0.72      0.74      1593\n",
      "\n",
      "    accuracy                           0.74      3194\n",
      "   macro avg       0.74      0.74      0.74      3194\n",
      "weighted avg       0.74      0.74      0.74      3194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_encoded, y_train)\n",
    "preds = rfc.predict(X_test_encoded)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.77      0.78      1601\n",
      "           1       0.77      0.79      0.78      1593\n",
      "\n",
      "    accuracy                           0.78      3194\n",
      "   macro avg       0.78      0.78      0.78      3194\n",
      "weighted avg       0.78      0.78      0.78      3194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression()\n",
    "logit.fit(X_train_encoded, y_train)\n",
    "preds = logit.predict(X_test_encoded)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['hour', 'target', 'text', 'tokenizer with lowercase',\n",
       "        'tokenizer with lowercase, handle stripping, and length reduction',\n",
       "        'tokenizer with lowercase and alpha',\n",
       "        'tokenizer with lowercase, alpha and emoji',\n",
       "        'tokenizer with lowercase, alpha, and no stop words',\n",
       "        'tokenizer with lowercase, alpha and emoji, and no stop words'],\n",
       "       dtype='object'),\n",
       " array([2, 0, 1, 3, 4, 5, 6, 7, 8]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the x_train data aligned on cleaned text corpus\n",
    "X_train, X_test, y_train, y_test = load_splits_from_parquet(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    col_name=\"tokenizer with lowercase, handle stripping, and length reduction\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "X_train_encoded = model.encode(X_train.values, device=\"cpu\")\n",
    "X_test_encoded = model.encode(X_test.values, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.74      0.74      1601\n",
      "           1       0.74      0.74      0.74      1593\n",
      "\n",
      "    accuracy                           0.74      3194\n",
      "   macro avg       0.74      0.74      0.74      3194\n",
      "weighted avg       0.74      0.74      0.74      3194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_encoded, y_train)\n",
    "preds = rfc.predict(X_test_encoded)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77      1601\n",
      "           1       0.77      0.77      0.77      1593\n",
      "\n",
      "    accuracy                           0.77      3194\n",
      "   macro avg       0.77      0.77      0.77      3194\n",
      "weighted avg       0.77      0.77      0.77      3194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression()\n",
    "logit.fit(X_train_encoded, y_train)\n",
    "preds = logit.predict(X_test_encoded)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([X_train, y_train], axis=1).to_parquet(\"../data/processed/X_train.parquet\")\n",
    "pd.concat([X_test, y_test], axis=1).to_parquet(\"../data/processed/X_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b52eeff67a8407c892bcdeceb485989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeec664628cc4defa05eee4f424a869d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\n",
    "        \"train\": \"../data/processed/X_train.parquet\",\n",
    "        \"test\": \"../data/processed/X_test.parquet\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3dbad29d40e4e1e836f0e0bebc4e462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12772 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f027057a9429478f9e507aa2a6fdda0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3194 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"google-bert/bert-base-cased\", num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Output directory\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    per_device_train_batch_size=8,  # Batch size for training\n",
    "    per_device_eval_batch_size=8,  # Batch size for evaluation\n",
    "    warmup_steps=500,  # Number of warmup steps\n",
    "    weight_decay=0.01,  # Strength of weight decay\n",
    "    logging_dir=\"./logs\",  # Directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,  # The instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,  # Training arguments, defined above\n",
    "    train_dataset=tokenized_datasets[\"train\"],  # Training dataset\n",
    "    tokenizer=tokenizer,  # Tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
